import json
import argparse

from eval_models import get_eval_model
from utils import check_format


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Calculate ASR using the specified evaluation model."
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        required=True,
        help="Path to the benchmark JSON file to calculate ASR for."
    )
    parser.add_argument(
        "--save-dir",
        type=str,
        required=True,
        help="Path to save the scored output JSON file."
    )
    parser.add_argument(
        "--scorer",
        type=str,
        choices=["gpt4judge", "harmscore"],
        required=True,
        help="Evaluation method to calculate ASR with (gpt4judge or harmscore)."
    )
    args = parser.parse_args()

    if not check_format(args.data_dir):
        raise ValueError(f"Data file '{args.data_dir}' not properly formatted")

    eval_model = get_eval_model(args.scorer)

    with open(args.data_dir, "r") as infile:
        data = json.load(infile)

    data_scored = eval_model.compute_scores(data)

    with open(args.save_dir, "w") as outfile:
        json.dump(data_scored, outfile, indent=4)


if __name__ == "__main__":
    main()
