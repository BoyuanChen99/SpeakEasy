import os
import yaml
import time
import multiprocessing

import openai
from box import Box
from tqdm import tqdm
from openai import OpenAI

from utils import CHAT_MODELS, partition


def get_prompt_response(instance, model, temperature, max_tokens, top_p):
    """
    Generate a prompt response using the OpenAI API.

    Depending on whether the specified model is a chat model (as determined by CHAT_MODELS),
    this function sends the prompt via the appropriate OpenAI API endpoint and returns the generated response.

    Args:
        instance (str): The input prompt.
        model (str): The model name to use.
        temperature (float): The temperature parameter for generation.
        max_tokens (int): The maximum number of tokens to generate.
        top_p (float): The nucleus sampling parameter.

    Returns:
        str: The generated response text.
    """
    chat_model = any(chat_model_name in model for chat_model_name in CHAT_MODELS)

    if chat_model:
        response = (
            client.chat.completions.create(
                messages=[{"role": "user", "content": instance}],
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p
            )
            .choices[0]
            .message.content
        )
    else:
        response = (
            client.completions.create(
                model=model, prompt=instance, temperature=temperature
            )
            .choices[0]
            .text
        )
    return response


def query_worker(model, inputs, process_id, lock, temperature, max_tokens, top_p):
    """
    Worker function for processing a batch of input prompts via the OpenAI API.

    This function processes a list of inputs by querying the API (using a nested query_loop)
    while updating a progress bar. It handles retries on various API errors and returns the list of responses.

    Args:
        model (str): The model name to use for generation.
        inputs (list): A list of input prompt strings (or lists of strings).
        process_id (int): The identifier for the current process.
        lock (multiprocessing.Lock): A lock object to manage concurrent access to shared resources.
        temperature (float): The temperature parameter for generation.
        max_tokens (int): The maximum number of tokens to generate.
        top_p (float): The nucleus sampling parameter.

    Returns:
        list: A list of generated responses corresponding to the inputs.
    """
    def query_loop(model, instance, temperature, max_tokens, top_p):
        """
        Sends a single prompt instance to the OpenAI API and returns the generated response.

        Handles retries for various API errors such as internal server errors, rate limits, and timeouts.
        If the instance is an empty string, returns an empty string immediately.

        Args:
            model (str): The model name to use.
            instance (str): The prompt instance.
            temperature (float): The temperature parameter for generation.
            max_tokens (int): The maximum number of tokens to generate.
            top_p (float): The nucleus sampling parameter.

        Returns:
            str: The generated response text.
        """
        # Edge case: If an empty string is passed, return an empty string
        if instance == "":
            return ""

        server_error = False
        response = None
        while response is None:
            try:
                response = get_prompt_response(instance, model, temperature, max_tokens, top_p)
            except openai.InternalServerError:
                # If first time encountering error, change model to long-context version
                if not server_error:
                    server_error = True
                    model += "-16k"
                # Otherwise, adding prompt did not help, so exit
                else:
                    print("[InternalServerError]: Likely generated invalid Unicode output.")
                    print(instance)
                    exit()
            except openai.BadRequestError:
                print("[BadAPIRequest]: Likely input too long or invalid settings (e.g. temperature > 2).")
                print(instance)
            except openai.RateLimitError:
                print("[RateLimitError]: Too many requests. Retrying after 30 seconds...")
                time.sleep(30)
                continue
            except openai.Timeout:
                continue

        return response

    with lock:
        bar = tqdm(
            desc=f"Process {process_id+1}",
            total=len(inputs),
            position=process_id + 1,
            leave=False,
        )

    # If a partially populated results file exists, load and continue (not implemented here)
    responses = list()
    start_index = len(responses)

    for instance in inputs[start_index:]:
        with lock:
            bar.update(1)

        # If instance is a list, process each element; otherwise, process the instance directly
        if isinstance(instance, list):
            response = [
                query_loop(model, instance_item, temperature, max_tokens, top_p)
                if instance_item != "" else ""
                for instance_item in instance
            ]
        elif isinstance(instance, str):
            response = query_loop(model, instance, temperature, max_tokens, top_p)

        responses.append(response)

    with lock:
        bar.close()

    return responses


class OpenAIModel:
    def __init__(self, model_name):
        """
        Initializes the OpenAIModel with the specified model name and configuration.

        Loads the API key and generation parameters from 'config.yaml' located in the same directory
        as this script, and initializes the global OpenAI client.

        Args:
            model_name (str): The name of the model to use for inference.
        """
        global client
        args = Box(yaml.safe_load(open(os.path.join(os.path.dirname(__file__), "config.yaml"), "r")))
        client = OpenAI(api_key=args.api_key)

        self.model = model_name
        self.num_processes = args.num_processes
        self.temperature = args.temperature
        self.max_tokens = args.max_tokens
        self.top_p = args.top_p

    def infer_batch(self, inputs):
        """
        Performs batch inference on a list of inputs using multiprocessing.

        Partitions the input prompts across multiple processes and queries the OpenAI API concurrently
        using the query_worker function. Collects and returns the combined responses from all processes.

        Args:
            inputs (list): A list of input prompt strings.

        Returns:
            list: A list of responses generated by the model.
        """
        # Partition instances
        paritioned_inputs = partition(inputs, self.num_processes)

        # Start multiprocess instances
        lock = multiprocessing.Manager().Lock()
        pool = multiprocessing.Pool(processes=self.num_processes)

        worker_results = []
        for process_id in range(len(paritioned_inputs)):
            async_args = (
                self.model,
                paritioned_inputs[process_id],
                process_id,
                lock,
                self.temperature,
                self.max_tokens,
                self.top_p
            )
            # Run each worker
            worker_results.append(pool.apply_async(query_worker, args=async_args))

        pool.close()
        pool.join()

        responses = []
        for worker_result in worker_results:
            responses += worker_result.get()

        return responses
