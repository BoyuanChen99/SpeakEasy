import os
import json

import ray
import yaml
import torch 
from box import Box
from tqdm import tqdm
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


class vLLMModel:
    def __init__(self, model_name: str) -> None:
        """
        Initializes the vLLMModel instance.

        Loads configuration from 'config.yaml', determines the number of available GPUs,
        initializes the tokenizer, sets up sampling parameters, and creates the LLM model instance.
        If multiple GPUs are available, Ray is initialized for tensor parallelism.

        Args:
            model_name (str): The name or path of the model to load.
        """
        config_path = os.path.join(os.path.dirname(__file__), "config.yaml")
        with open(config_path, "r") as file:
            config = Box(yaml.safe_load(file))
            
        num_gpus = torch.cuda.device_count()

        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        if num_gpus > 1:
            ray.init(ignore_reinit_error=True)
            
        self.temperature = config.temperature
        self.top_p = config.top_p
        self.max_tokens = config.max_tokens

        self.sampling_params = SamplingParams(
            temperature=self.temperature,
            top_p=self.top_p,
            max_tokens=self.max_tokens
        )

        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=num_gpus,
            gpu_memory_utilization=0.95,
            dtype=torch.float16,
            max_model_len=4096
        )

    def infer_batch(self, inputs: list, save_dir: str, batch_size: int = 1) -> list:
        """
        Performs batch inference on the input data and saves the responses.

        Processes the input prompts in batches, applies a chat template to format the prompt,
        generates outputs using the vLLM model, and appends the responses to a file at `save_dir`.
        If a responses file already exists, it is loaded and new responses are appended.

        Args:
            inputs (list): Input data as a list of prompt strings.
            save_dir (str): The file path where the output responses are saved (JSON file).
            batch_size (int, optional): Number of samples to process in a batch. Defaults to 1.

        Returns:
            list: The complete list of generated responses.
        """
        responses = (
            json.load(open(save_dir, "r")) if os.path.exists(save_dir) else list()
        )
        start_index = len(responses)

        for i in tqdm(range(start_index, len(inputs), batch_size), desc="Processing"):
            batch_inputs = inputs[i:i + batch_size]
            messages = [
                {"role": "system", "content": ""},
                {"role": "user", "content": batch_inputs[0]}
            ]

            prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            try:
                outputs = self.llm.generate([prompt], self.sampling_params)
            except Exception as e:
                print(f"Error during generation: {e}")
                outputs = None

            # Parse outputs and append responses
            for output_idx, input_text in enumerate(batch_inputs):
                if outputs and output_idx < len(outputs) and outputs[output_idx].outputs:
                    responses.append(outputs[output_idx].outputs[0].text)
                else:
                    responses.append("")

            json.dump(responses, open(save_dir, "w"), indent=4)

        return responses
