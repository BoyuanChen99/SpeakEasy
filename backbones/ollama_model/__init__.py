import os
import json

import yaml
import ollama
from box import Box
from tqdm import tqdm


class OllamaModel:
    def __init__(self, model_name: str) -> None:
        """
        Initializes the OllamaModel instance.

        Loads the configuration from 'config.yaml' in the same directory as this script
        and sets model generation parameters such as num_predict, temperature, and top_p.

        Args:
            model_name (str): The name of the Ollama model to use.
        """
        self.model = model_name

        config_path = os.path.join(os.path.dirname(__file__), "config.yaml")
        params = Box(yaml.safe_load(open(config_path, "r")))

        self.num_predict = params.num_predict
        self.temperature = params.temperature
        self.top_p = params.top_p

    def infer_batch(self, inputs: list, save_dir: str) -> list:
        """
        Performs batch inference on input prompts and saves the generated responses.

        For each input prompt (or list of prompts), the function generates a response using
        the Ollama model. Generated responses are appended to a list and saved to the file
        specified by `save_dir`.

        Args:
            inputs (list): A list of input prompts. Each element can be a string or a list of strings.
            save_dir (str): The file path to save the output responses (in JSON format).

        Returns:
            list: The complete list of generated responses.
        """
        responses = json.load(open(save_dir, "r")) if os.path.exists(save_dir) else list()
        start_index = len(responses)

        for instance in tqdm(inputs[start_index:], ascii=True, leave=False):
            if isinstance(instance, list):
                response = [
                    (
                        ollama.generate(
                            model=self.model,
                            prompt=instance_item,
                            options={
                                "num_predict": self.num_predict,
                                "top_p": self.top_p,
                                "temperature": self.temperature,
                            },
                        )["response"]
                        if instance_item != ""
                        else ""
                    )
                    for instance_item in instance
                ]
            elif isinstance(instance, str):
                response = ollama.generate(model=self.model, prompt=instance)["response"]

            responses.append(response)
            json.dump(responses, open(save_dir, "w"), indent=4)

        return responses
